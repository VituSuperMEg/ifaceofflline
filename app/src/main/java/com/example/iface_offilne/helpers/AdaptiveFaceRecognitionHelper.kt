package com.example.iface_offilne.helpers

import android.content.Context
import android.graphics.Bitmap
import android.graphics.Rect
import android.util.Log
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetectorOptions
import com.google.mlkit.vision.face.FaceLandmark
import com.google.mlkit.vision.face.Face
import org.tensorflow.lite.Interpreter
import java.nio.ByteBuffer
import java.nio.ByteOrder
import kotlin.math.abs
import kotlin.math.sqrt
import kotlinx.coroutines.tasks.await
import com.example.iface_offilne.helpers.PerformanceLevel


/**
 * üöÄ HELPER ADAPTATIVO PARA RECONHECIMENTO FACIAL
 * 
 * Sistema inteligente que ajusta automaticamente os par√¢metros
 * de reconhecimento facial baseado nas capacidades do dispositivo
 * 
 * ‚úÖ Funciona em dispositivos de baixo desempenho
 * ‚úÖ Mant√©m precis√£o em dispositivos potentes
 * ‚úÖ Otimiza√ß√£o autom√°tica de performance
 */
class AdaptiveFaceRecognitionHelper(private val context: Context) {
    
    companion object {
        private const val TAG = "AdaptiveFaceRecognition"
    }
    
    private val deviceCapabilityHelper = DeviceCapabilityHelper(context)
    private val adaptiveConfig = deviceCapabilityHelper.getAdaptiveFaceRecognitionConfig()
    private val deviceInfo = deviceCapabilityHelper.getDeviceInfo()
    
    private val faceDetector = FaceDetection.getClient(
        FaceDetectorOptions.Builder()
            .setPerformanceMode(
                when (adaptiveConfig.imageQuality) {
                    DeviceCapabilityHelper.ImageQuality.LOW -> FaceDetectorOptions.PERFORMANCE_MODE_FAST
                    DeviceCapabilityHelper.ImageQuality.MEDIUM -> FaceDetectorOptions.PERFORMANCE_MODE_FAST
                    DeviceCapabilityHelper.ImageQuality.HIGH -> FaceDetectorOptions.PERFORMANCE_MODE_ACCURATE
                }
            )
            .setLandmarkMode(
                when (adaptiveConfig.imageQuality) {
                    DeviceCapabilityHelper.ImageQuality.LOW -> FaceDetectorOptions.LANDMARK_MODE_NONE
                    DeviceCapabilityHelper.ImageQuality.MEDIUM -> FaceDetectorOptions.LANDMARK_MODE_ALL
                    DeviceCapabilityHelper.ImageQuality.HIGH -> FaceDetectorOptions.LANDMARK_MODE_ALL
                }
            )
            .setClassificationMode(
                when (adaptiveConfig.imageQuality) {
                    DeviceCapabilityHelper.ImageQuality.LOW -> FaceDetectorOptions.CLASSIFICATION_MODE_NONE
                    DeviceCapabilityHelper.ImageQuality.MEDIUM -> FaceDetectorOptions.CLASSIFICATION_MODE_ALL
                    DeviceCapabilityHelper.ImageQuality.HIGH -> FaceDetectorOptions.CLASSIFICATION_MODE_ALL
                }
            )
            .setMinFaceSize(
                when (adaptiveConfig.imageQuality) {
                    DeviceCapabilityHelper.ImageQuality.LOW -> 0.05f
                    DeviceCapabilityHelper.ImageQuality.MEDIUM -> 0.1f
                    DeviceCapabilityHelper.ImageQuality.HIGH -> 0.15f
                }
            )
            .build()
    )
    
    private var interpreter: Interpreter? = null
    private var modelLoaded = false
    
    init {
        Log.d(TAG, "üöÄ === INICIANDO HELPER ADAPTATIVO ===")
        Log.d(TAG, "üìä Dispositivo: ${deviceInfo.performanceLevel} (Score: ${String.format("%.1f", deviceInfo.deviceScore)}/100)")
        Log.d(TAG, "üíæ Mem√≥ria: ${String.format("%.1f", deviceInfo.memoryInfo.totalGB)}GB")
        Log.d(TAG, "üñ•Ô∏è CPU: ${deviceInfo.cpuInfo.cores} cores")
        Log.d(TAG, "ü§ñ Android: ${deviceInfo.androidRelease} (API ${deviceInfo.androidVersion})")
        
        loadTensorFlowModel()
    }
    
    /**
     * üîç RECONHECIMENTO FACIAL ADAPTATIVO
     * Ajusta automaticamente a qualidade baseado no dispositivo
     */
    suspend fun recognizeFaceAdaptive(bitmap: Bitmap): FaceRecognitionResult {
        return try {
            Log.d(TAG, "üîç === RECONHECIMENTO FACIAL ADAPTATIVO ===")
            Log.d(TAG, "üéõÔ∏è Configura√ß√£o: ${deviceInfo.performanceLevel}")
            Log.d(TAG, "üìä Thresholds: Similaridade>=${adaptiveConfig.minSimilarityThreshold}, Dist√¢ncia<=${adaptiveConfig.maxEuclideanDistance}, Confian√ßa>=${adaptiveConfig.requiredConfidence}")
            
            // ‚úÖ 1. VALIDA√á√ÉO DE QUALIDADE ADAPTATIVA
            val qualityCheck = validateImageQualityAdaptive(bitmap)
            if (!qualityCheck.isValid) {
                Log.w(TAG, "‚ùå Qualidade insuficiente: ${qualityCheck.reason}")
                return FaceRecognitionResult.Failure(qualityCheck.reason)
            }
            
            // ‚úÖ 2. DETEC√á√ÉO DE FACE ADAPTATIVA
            val faceValidation = validateFaceDetectionAdaptive(bitmap)
            if (!faceValidation.isValid) {
                Log.w(TAG, "‚ùå Face n√£o v√°lida: ${faceValidation.reason}")
                return FaceRecognitionResult.Failure(faceValidation.reason)
            }
            
            // ‚úÖ 3. PROCESSAMENTO DE IMAGEM ADAPTATIVO
            val processedBitmap = processImageAdaptive(bitmap)
            if (processedBitmap == null) {
                Log.e(TAG, "‚ùå Falha no processamento de imagem")
                return FaceRecognitionResult.Failure("Falha no processamento de imagem")
            }
            
            // ‚úÖ 4. GERA√á√ÉO DO EMBEDDING
            val embedding = generateFaceEmbedding(processedBitmap)
            if (embedding == null) {
                Log.e(TAG, "‚ùå Falha ao gerar embedding")
                return FaceRecognitionResult.Failure("Falha ao processar face")
            }
            
            // ‚úÖ 5. RECONHECIMENTO ADAPTATIVO
            val recognitionResult = performAdaptiveRecognition(embedding)
            
            Log.d(TAG, "‚úÖ Reconhecimento adaptativo conclu√≠do!")
            recognitionResult
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro no reconhecimento adaptativo", e)
            FaceRecognitionResult.Failure("Erro interno: ${e.message}")
        }
    }
    
    /**
     * üéØ RECONHECIMENTO ADAPTATIVO COM CONFIGURA√á√ïES DIN√ÇMICAS
     */
    private suspend fun performAdaptiveRecognition(embedding: FloatArray): FaceRecognitionResult {
        return try {
            // ‚úÖ Carregar dados do banco
            val db = com.example.iface_offilne.data.AppDatabase.getInstance(context)
            val faceDao = db.faceDao()
            val funcionarioDao = db.usuariosDao()
            
            val faces = faceDao.getAllFaces()
            if (faces.isEmpty()) {
                return FaceRecognitionResult.Failure("Nenhuma face cadastrada")
            }
            
            Log.d(TAG, "üîç Comparando com ${faces.size} faces cadastradas")
            
            // ‚úÖ VARI√ÅVEIS PARA MELHOR MATCH
            var bestMatch: com.example.iface_offilne.data.FuncionariosEntity? = null
            var bestSimilarity = 0f
            var bestEuclideanDistance = Float.MAX_VALUE
            var bestConfidence = 0f
            
            // ‚úÖ COMPARA√á√ÉO ADAPTATIVA
            for (face in faces) {
                try {
                    val storedEmbedding = parseEmbedding(face.embedding)
                    if (storedEmbedding == null) {
                        Log.w(TAG, "‚ö†Ô∏è Embedding inv√°lido para funcion√°rio ${face.funcionarioId}")
                        continue
                    }
                    
                    // ‚úÖ Calcular m√©tricas
                    val cosineSimilarity = calculateCosineSimilarity(embedding, storedEmbedding)
                    val euclideanDistance = calculateEuclideanDistance(embedding, storedEmbedding)
                    val confidence = (cosineSimilarity + (1f - euclideanDistance)) / 2f
                    
                    Log.d(TAG, "üë§ Funcion√°rio ${face.funcionarioId}: Similaridade=${String.format("%.3f", cosineSimilarity)}, Dist√¢ncia=${String.format("%.3f", euclideanDistance)}, Confian√ßa=${String.format("%.3f", confidence)}")
                    
                    // ‚úÖ VALIDA√á√ÉO ADAPTATIVA: Usar thresholds baseados no dispositivo
                    if (cosineSimilarity >= adaptiveConfig.minSimilarityThreshold && 
                        euclideanDistance <= adaptiveConfig.maxEuclideanDistance && 
                        confidence >= adaptiveConfig.requiredConfidence) {
                        
                        // ‚úÖ Se encontrou uma correspond√™ncia v√°lida, verificar se √© melhor
                        if (confidence > bestConfidence) {
                            val funcionarioId = face.funcionarioId.toIntOrNull()
                            bestMatch = if (funcionarioId != null) {
                                funcionarioDao.getUsuario().find { it.id == funcionarioId }
                            } else {
                                funcionarioDao.getUsuario().find { it.codigo == face.funcionarioId }
                            }
                            bestSimilarity = cosineSimilarity
                            bestEuclideanDistance = euclideanDistance
                            bestConfidence = confidence
                            
                            Log.d(TAG, "üéØ NOVO MELHOR MATCH: ${bestMatch?.nome} (Confian√ßa: ${String.format("%.3f", confidence)})")
                        }
                    }
                    
                } catch (e: Exception) {
                    Log.e(TAG, "‚ùå Erro ao comparar com face ${face.funcionarioId}: ${e.message}")
                }
            }
            
            // ‚úÖ RESULTADO FINAL
            if (bestMatch != null) {
                Log.d(TAG, "‚úÖ RECONHECIMENTO BEM-SUCEDIDO!")
                Log.d(TAG, "üë§ Funcion√°rio: ${bestMatch.nome}")
                Log.d(TAG, "üìä M√©tricas: Similaridade=${String.format("%.3f", bestSimilarity)}, Dist√¢ncia=${String.format("%.3f", bestEuclideanDistance)}, Confian√ßa=${String.format("%.3f", bestConfidence)}")
                
                return FaceRecognitionResult.Success(
                    funcionario = bestMatch,
                    similarity = bestSimilarity,
                    euclideanDistance = bestEuclideanDistance,
                    confidence = bestConfidence
                )
            } else {
                Log.w(TAG, "‚ùå NENHUM FUNCION√ÅRIO RECONHECIDO")
                Log.w(TAG, "üìä Thresholds n√£o atendidos: Similaridade>=${adaptiveConfig.minSimilarityThreshold}, Dist√¢ncia<=${adaptiveConfig.maxEuclideanDistance}, Confian√ßa>=${adaptiveConfig.requiredConfidence}")
                return FaceRecognitionResult.Failure("Funcion√°rio n√£o reconhecido - thresholds adaptativos n√£o atendidos")
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro no reconhecimento adaptativo", e)
            return FaceRecognitionResult.Failure("Erro no reconhecimento: ${e.message}")
        }
    }
    
    /**
     * üñºÔ∏è PROCESSAMENTO DE IMAGEM ADAPTATIVO
     */
    private fun processImageAdaptive(bitmap: Bitmap): Bitmap? {
        return try {
            Log.d(TAG, "üñºÔ∏è Processando imagem adaptativamente...")
            
            // ‚úÖ Redimensionar baseado na qualidade do dispositivo
            val targetSize = adaptiveConfig.maxImageSize
            val processedBitmap = if (bitmap.width != targetSize || bitmap.height != targetSize) {
                Log.d(TAG, "üìè Redimensionando de ${bitmap.width}x${bitmap.height} para ${targetSize}x${targetSize}")
                Bitmap.createScaledBitmap(bitmap, targetSize, targetSize, true)
            } else {
                bitmap
            }
            
            Log.d(TAG, "‚úÖ Imagem processada: ${processedBitmap.width}x${processedBitmap.height}")
            processedBitmap
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro no processamento de imagem: ${e.message}")
            null
        }
    }
    
    /**
     * üìê VALIDA√á√ÉO DE QUALIDADE ADAPTATIVA
     */
    private suspend fun validateImageQualityAdaptive(bitmap: Bitmap): QualityCheckResult {
        return try {
            // ‚úÖ Verificar tamanho m√≠nimo
            if (bitmap.width < 100 || bitmap.height < 100) {
                return QualityCheckResult(false, "Imagem muito pequena (${bitmap.width}x${bitmap.height})")
            }
            
            // ‚úÖ Verificar se n√£o est√° reciclada
            if (bitmap.isRecycled) {
                return QualityCheckResult(false, "Imagem reciclada")
            }
            
            // ‚úÖ Verificar brilho e contraste (mais permissivo para dispositivos fracos)
            val brightness = calculateBrightness(bitmap)
            val contrast = calculateContrast(bitmap)
            
            Log.d(TAG, "üìä Qualidade: Brilho=${String.format("%.3f", brightness)}, Contraste=${String.format("%.3f", contrast)}")
            
            if (brightness < adaptiveConfig.minBrightness || brightness > adaptiveConfig.maxBrightness) {
                return QualityCheckResult(false, "Brilho inadequado (${String.format("%.3f", brightness)})")
            }
            
            if (contrast < adaptiveConfig.minContrast) {
                return QualityCheckResult(false, "Contraste baixo (${String.format("%.3f", contrast)})")
            }
            
            QualityCheckResult(true, "Qualidade adequada")
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro na valida√ß√£o de qualidade: ${e.message}")
            QualityCheckResult(false, "Erro na valida√ß√£o: ${e.message}")
        }
    }
    
    /**
     * üë§ VALIDA√á√ÉO DE DETEC√á√ÉO DE FACE ADAPTATIVA
     */
    private suspend fun validateFaceDetectionAdaptive(bitmap: Bitmap): FaceValidationResult {
        return try {
            val image = InputImage.fromBitmap(bitmap, 0)
            val faces = faceDetector.process(image).await()
            
            if (faces.isEmpty()) {
                return FaceValidationResult(false, "Nenhuma face detectada", null)
            }
            
            if (faces.size > 1) {
                return FaceValidationResult(false, "M√∫ltiplas faces detectadas", null)
            }
            
            val face = faces[0]
            
            // ‚úÖ Verificar tamanho da face (adaptativo)
            val faceRatio = calculateFaceRatio(face.boundingBox, bitmap.width, bitmap.height)
            Log.d(TAG, "üìê Propor√ß√£o da face: ${String.format("%.3f", faceRatio)}")
            
            if (faceRatio < adaptiveConfig.minFaceSizeRatio || faceRatio > adaptiveConfig.maxFaceSizeRatio) {
                return FaceValidationResult(false, "Face muito pequena ou muito grande (${String.format("%.1f", faceRatio * 100)}%)", face)
            }
            
            // ‚úÖ Verificar landmarks (mais permissivo para dispositivos fracos)
            if (adaptiveConfig.imageQuality != DeviceCapabilityHelper.ImageQuality.LOW) {
                val leftEye = face.getLandmark(FaceLandmark.LEFT_EYE)
                val rightEye = face.getLandmark(FaceLandmark.RIGHT_EYE)
                
                if (leftEye == null || rightEye == null) {
                    return FaceValidationResult(false, "Olhos n√£o detectados", face)
                }
                
                val eyeDistance = calculateEyeDistance(face)
                Log.d(TAG, "üëÄ Dist√¢ncia entre olhos: ${String.format("%.1f", eyeDistance)}px")
                
                if (eyeDistance < adaptiveConfig.minEyeDistance) {
                    return FaceValidationResult(false, "Olhos muito pr√≥ximos (${String.format("%.1f", eyeDistance)}px)", face)
                }
            }
            
            return FaceValidationResult(true, "Face v√°lida", face)
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro na valida√ß√£o de face: ${e.message}")
            return FaceValidationResult(false, "Erro na detec√ß√£o: ${e.message}", null)
        }
    }
    
    /**
     * ü§ñ CARREGAR MODELO TENSORFLOW ADAPTATIVO
     */
    private fun loadTensorFlowModel() {
        Log.d(TAG, "ü§ñ Carregando modelo TensorFlow adaptativo...")
        
        try {
            // ‚úÖ Usar configura√ß√µes otimizadas para o dispositivo
            val options = Interpreter.Options().apply {
                setNumThreads(
                    when (deviceInfo.performanceLevel) {
                        PerformanceLevel.LOW -> 1
                        PerformanceLevel.MEDIUM -> 2
                        PerformanceLevel.HIGH -> 4
                        else -> 2 // Fallback para casos inesperados
                    }
                )
                setUseNNAPI(adaptiveConfig.useTensorFlowOptimizations)
                setAllowFp16PrecisionForFp32(adaptiveConfig.useTensorFlowOptimizations)
                setAllowBufferHandleOutput(false)
            }
            
            // ‚úÖ Carregar modelo
            val modelFile = try {
                context.assets.open("facenet_model.tflite")
            } catch (e: Exception) {
                context.resources.openRawResource(com.example.iface_offilne.R.raw.mobilefacenet)
            }
            
            val modelBuffer = modelFile.use { input ->
                val bytes = ByteArray(input.available())
                input.read(bytes)
                ByteBuffer.allocateDirect(bytes.size).apply {
                    order(ByteOrder.nativeOrder())
                    put(bytes)
                    rewind()
                }
            }
            
            interpreter = Interpreter(modelBuffer, options)
            modelLoaded = true
            
            Log.d(TAG, "‚úÖ Modelo TensorFlow carregado com configura√ß√µes adaptativas")
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro ao carregar modelo TensorFlow: ${e.message}")
            modelLoaded = false
        }
    }
    
    /**
     * üî¢ GERAR EMBEDDING FACIAL
     */
    private fun generateFaceEmbedding(bitmap: Bitmap): FloatArray? {
        return try {
            if (!modelLoaded || interpreter == null) {
                Log.e(TAG, "‚ùå Modelo n√£o carregado")
                return null
            }
            
            val inputTensor = convertBitmapToTensorInput(bitmap)
            val output = Array(1) { FloatArray(512) } // Tamanho padr√£o do embedding
            
            interpreter?.run(inputTensor, output)
            
            val embedding = output[0]
            Log.d(TAG, "‚úÖ Embedding gerado: ${embedding.size} dimens√µes")
            
            embedding
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro ao gerar embedding: ${e.message}")
            null
        }
    }
    
    /**
     * üîÑ CONVERTER BITMAP PARA TENSOR
     */
    private fun convertBitmapToTensorInput(bitmap: Bitmap): ByteBuffer {
        val inputSize = 160 // Tamanho padr√£o do modelo
        val bufferSize = 4 * inputSize * inputSize * 3
        
        val byteBuffer = ByteBuffer.allocateDirect(bufferSize)
        byteBuffer.order(ByteOrder.nativeOrder())
        
        val resizedBitmap = if (bitmap.width != inputSize || bitmap.height != inputSize) {
            Bitmap.createScaledBitmap(bitmap, inputSize, inputSize, true)
        } else {
            bitmap
        }
        
        val intValues = IntArray(inputSize * inputSize)
        resizedBitmap.getPixels(intValues, 0, inputSize, 0, 0, inputSize, inputSize)
        
        for (pixel in intValues) {
            val r = ((pixel shr 16) and 0xFF) / 127.5f - 1.0f
            val g = ((pixel shr 8) and 0xFF) / 127.5f - 1.0f
            val b = (pixel and 0xFF) / 127.5f - 1.0f
            
            byteBuffer.putFloat(r)
            byteBuffer.putFloat(g)
            byteBuffer.putFloat(b)
        }
        
        if (resizedBitmap != bitmap) {
            resizedBitmap.recycle()
        }
        
        return byteBuffer
    }
    
    // ========== M√âTODOS AUXILIARES ==========
    
    private fun calculateCosineSimilarity(embedding1: FloatArray, embedding2: FloatArray): Float {
        if (embedding1.size != embedding2.size) return 0f
        
        var dotProduct = 0f
        var norm1 = 0f
        var norm2 = 0f
        
        for (i in embedding1.indices) {
            dotProduct += embedding1[i] * embedding2[i]
            norm1 += embedding1[i] * embedding1[i]
            norm2 += embedding2[i] * embedding2[i]
        }
        
        val denominator = sqrt(norm1) * sqrt(norm2)
        return if (denominator > 0f) dotProduct / denominator else 0f
    }
    
    private fun calculateEuclideanDistance(embedding1: FloatArray, embedding2: FloatArray): Float {
        if (embedding1.size != embedding2.size) return Float.MAX_VALUE
        
        var sum = 0f
        for (i in embedding1.indices) {
            val diff = embedding1[i] - embedding2[i]
            sum += diff * diff
        }
        
        return sqrt(sum)
    }
    
    private fun parseEmbedding(embeddingString: String): FloatArray? {
        return try {
            embeddingString.split(",").map { it.trim().toFloat() }.toFloatArray()
        } catch (e: Exception) {
            null
        }
    }
    
    private fun calculateFaceRatio(boundingBox: Rect, imageWidth: Int, imageHeight: Int): Float {
        val faceArea = boundingBox.width() * boundingBox.height()
        val imageArea = imageWidth * imageHeight
        return faceArea.toFloat() / imageArea.toFloat()
    }
    
    private fun calculateEyeDistance(face: Face): Float {
        val leftEye = face.getLandmark(FaceLandmark.LEFT_EYE)
        val rightEye = face.getLandmark(FaceLandmark.RIGHT_EYE)
        
        if (leftEye == null || rightEye == null) return 0f
        
        val dx = leftEye.position.x - rightEye.position.x
        val dy = leftEye.position.y - rightEye.position.y
        return sqrt(dx * dx + dy * dy)
    }
    
    private fun calculateBrightness(bitmap: Bitmap): Float {
        val pixels = IntArray(bitmap.width * bitmap.height)
        bitmap.getPixels(pixels, 0, bitmap.width, 0, 0, bitmap.width, bitmap.height)
        
        var totalBrightness = 0f
        for (pixel in pixels) {
            val r = (pixel shr 16) and 0xFF
            val g = (pixel shr 8) and 0xFF
            val b = pixel and 0xFF
            totalBrightness += (r + g + b) / 3f / 255f
        }
        
        return totalBrightness / pixels.size
    }
    
    private fun calculateContrast(bitmap: Bitmap): Float {
        val pixels = IntArray(bitmap.width * bitmap.height)
        bitmap.getPixels(pixels, 0, bitmap.width, 0, 0, bitmap.width, bitmap.height)
        
        var totalBrightness = 0f
        for (pixel in pixels) {
            val r = (pixel shr 16) and 0xFF
            val g = (pixel shr 8) and 0xFF
            val b = pixel and 0xFF
            totalBrightness += (r + g + b) / 3f / 255f
        }
        
        val averageBrightness = totalBrightness / pixels.size
        
        var variance = 0f
        for (pixel in pixels) {
            val r = (pixel shr 16) and 0xFF
            val g = (pixel shr 8) and 0xFF
            val b = pixel and 0xFF
            val brightness = (r + g + b) / 3f / 255f
            variance += (brightness - averageBrightness) * (brightness - averageBrightness)
        }
        
        return sqrt(variance / pixels.size)
    }
    
    // ========== CLASSES DE RESULTADO ==========
    
    sealed class FaceRecognitionResult {
        data class Success(
            val funcionario: com.example.iface_offilne.data.FuncionariosEntity,
            val similarity: Float,
            val euclideanDistance: Float,
            val confidence: Float
        ) : FaceRecognitionResult()
        data class Failure(val reason: String) : FaceRecognitionResult()
    }
    
    data class QualityCheckResult(val isValid: Boolean, val reason: String)
    
    data class FaceValidationResult(val isValid: Boolean, val reason: String, val face: Face?)
} 