package com.example.iface_offilne

import android.Manifest
import android.content.pm.PackageManager
import android.graphics.*
import android.os.Bundle
import android.os.Environment
import android.util.Log
import android.view.View
import android.widget.FrameLayout
import android.widget.Toast
import androidx.appcompat.app.AppCompatActivity
import androidx.camera.core.*
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import com.example.iface_offilne.data.AppDatabase
import com.example.iface_offilne.data.FaceEntity
import com.example.iface_offilne.helpers.Helpers
import com.example.iface_offilne.helpers.bitmapToFloatArray
import com.example.iface_offilne.helpers.cropFace
import com.example.iface_offilne.helpers.fixImageOrientationDefinitive
import com.example.iface_offilne.helpers.toBitmap
import com.example.iface_offilne.models.FacesModel
import com.example.iface_offilne.models.FuncionariosLocalModel
import com.example.iface_offilne.util.FaceOverlayView
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetectorOptions
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.launch
import kotlinx.coroutines.withContext
import org.tensorflow.lite.Interpreter
import java.io.File
import java.io.FileInputStream
import java.io.FileOutputStream
import java.io.IOException
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.channels.FileChannel
import java.text.SimpleDateFormat
import java.util.*

class CameraActivity : AppCompatActivity() {

    private lateinit var previewView: PreviewView
    private lateinit var imageAnalyzer: ImageAnalysis
    private lateinit var overlay: FaceOverlayView

    private var interpreter: Interpreter? = null
    private var modelLoaded = false

    private var modelInputWidth = 112
    private var modelInputHeight = 112
    private var modelOutputSize = 192

    companion object {
        private const val REQUEST_CODE_PERMISSIONS = 10
        private val REQUIRED_PERMISSIONS = when {
            android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.UPSIDE_DOWN_CAKE -> {
                // Android 14+ (API 34+)
                arrayOf(
                    Manifest.permission.CAMERA,
                    Manifest.permission.READ_MEDIA_IMAGES,
                    Manifest.permission.POST_NOTIFICATIONS
                )
            }
            android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.TIRAMISU -> {
                // Android 13+ (API 33+)
                arrayOf(
                    Manifest.permission.CAMERA,
                    Manifest.permission.READ_MEDIA_IMAGES
                )
            }
            else -> {
                // Android 12 e abaixo
                arrayOf(
                    Manifest.permission.CAMERA,
                    Manifest.permission.WRITE_EXTERNAL_STORAGE,
                    Manifest.permission.READ_EXTERNAL_STORAGE
                )
            }
        }
        private const val TAG = "CameraActivity"

        // Assinatura de arquivo TFLite v√°lido
        private val TFLITE_SIGNATURE = byteArrayOf(0x54, 0x46, 0x4C, 0x33) // "TFL3"
    }

    private var faceDetector = FaceDetection.getClient(
        FaceDetectorOptions.Builder()
            .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_FAST)
            .setLandmarkMode(FaceDetectorOptions.LANDMARK_MODE_NONE)
            .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_NONE)
            .build()
    )

    private var alreadySaved = false
    private var faceDetectionCount = 0
    private var currentFaceBitmap: Bitmap? = null

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        setupUI()
        Log.d(TAG, "üöÄ === INICIANDO APLICA√á√ÉO ===")

        // Carrega o modelo
        loadTensorFlowModel()

        // Solicita permiss√µes
        Log.d(TAG, "üîê Verificando permiss√µes...")
        Log.d(TAG, "üì± Vers√£o Android: ${android.os.Build.VERSION.SDK_INT}")
        Log.d(TAG, "üìã Permiss√µes necess√°rias: ${REQUIRED_PERMISSIONS.joinToString(", ")}")
        
        if (allPermissionsGranted()) {
            Log.d(TAG, "‚úÖ Todas as permiss√µes j√° concedidas")
            startCamera()
        } else {
            Log.d(TAG, "‚ùå Permiss√µes pendentes - solicitando...")
            // Mostrar mensagem informativa antes de solicitar permiss√µes
            Toast.makeText(this, "üì∑ O app precisa de permiss√£o para c√¢mera e armazenamento para registrar sua face", Toast.LENGTH_LONG).show()
            ActivityCompat.requestPermissions(this, REQUIRED_PERMISSIONS, REQUEST_CODE_PERMISSIONS)
        }
    }

    private fun setupUI() {
        previewView = PreviewView(this).apply { id = View.generateViewId() }
        overlay = FaceOverlayView(this).apply { id = View.generateViewId() }

        val container = FrameLayout(this).apply {
            layoutParams = FrameLayout.LayoutParams(
                FrameLayout.LayoutParams.MATCH_PARENT,
                FrameLayout.LayoutParams.MATCH_PARENT
            )
            addView(previewView)
        }
        container.addView(overlay)
        setContentView(container)
    }

    private fun loadTensorFlowModel() {
        try {
            Log.d(TAG, "üìÇ === VERIFICA√á√ÉO DO MODELO ===")
            listAssetsFiles()

            // Verifica se o arquivo existe
            if (!checkModelExists()) {
                Log.w(TAG, "‚ö†Ô∏è  Arquivo model.tflite n√£o encontrado")
                createDummyModel()
                showToast("Funcionando sem modelo (apenas detec√ß√£o)")
                return
            }

            // Valida o arquivo
            if (!validateModelFile()) {
                Log.e(TAG, "‚ùå Arquivo model.tflite √© inv√°lido!")
                showToast("Arquivo model.tflite corrompido ou inv√°lido")
                return
            }

            // Tenta carregar
            val buffer = loadModelFile("model.tflite")
            Log.d(TAG, "‚úÖ Buffer carregado! Tamanho: ${buffer.capacity()} bytes")

            // Cria interpretador
            val options = Interpreter.Options().apply {
                setNumThreads(2)
                setUseNNAPI(false)
            }

            interpreter = Interpreter(buffer, options)
            interpreter?.allocateTensors()

            Log.d(TAG, "‚úÖ Interpretador criado e tensores alocados!")

            if (checkAndExtractModelDimensions()) {
                modelLoaded = true
                showToast("‚úÖ Modelo TensorFlow carregado!")
                Log.d(TAG, "üéØ Modelo pronto para uso!")
            } else {
                throw Exception("Dimens√µes do modelo inv√°lidas")
            }

        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro ao carregar modelo: ${e.javaClass.simpleName}", e)

            when {
                e.message?.contains("flatbuffer") == true -> {
                    Log.e(TAG, "üí° DIAGN√ìSTICO: Arquivo n√£o √© um modelo TFLite v√°lido")
                    showToast("‚ùå Arquivo n√£o √© um modelo TensorFlow Lite v√°lido")
                }
                e.message?.contains("not found") == true -> {
                    Log.e(TAG, "üí° DIAGN√ìSTICO: Arquivo model.tflite n√£o encontrado")
                    showToast("‚ö†Ô∏è  Arquivo model.tflite n√£o encontrado")
                }
                else -> {
                    Log.e(TAG, "üí° DIAGN√ìSTICO: Erro desconhecido no modelo")
                    showToast("‚ùå Erro no modelo: ${e.message}")
                }
            }

            interpreter?.close()
            interpreter = null
            modelLoaded = false

            // Funciona sem modelo
            Log.w(TAG, "üîÑ Continuando apenas com detec√ß√£o de faces...")
        }
    }

    private fun checkModelExists(): Boolean {
        return try {
            val files = assets.list("") ?: emptyArray()
            files.contains("model.tflite")
        } catch (e: Exception) {
            false
        }
    }

    private fun validateModelFile(): Boolean {
        return try {
            assets.open("model.tflite").use { inputStream ->
                val header = ByteArray(8)
                val bytesRead = inputStream.read(header)

                Log.d(TAG, "üîç Validando arquivo...")
                Log.d(TAG, "   Bytes lidos: $bytesRead")
                Log.d(TAG, "   Header: ${header.joinToString(" ") { "%02X".format(it) }}")

                // Verifica se tem pelo menos 8 bytes
                if (bytesRead < 8) {
                    Log.e(TAG, "‚ùå Arquivo muito pequeno (${bytesRead} bytes)")
                    return false
                }

                // Verifica assinatura TFLite (pode estar em diferentes posi√ß√µes)
                val isValidTFLite = header.sliceArray(0..3).contentEquals(TFLITE_SIGNATURE) ||
                        header.sliceArray(4..7).contentEquals(TFLITE_SIGNATURE)

                if (isValidTFLite) {
                    Log.d(TAG, "‚úÖ Assinatura TFLite v√°lida encontrada!")
                } else {
                    Log.e(TAG, "‚ùå Assinatura TFLite n√£o encontrada")
                    Log.e(TAG, "   Esperado: ${TFLITE_SIGNATURE.joinToString(" ") { "%02X".format(it) }}")
                }

                isValidTFLite
            }
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro ao validar arquivo", e)
            false
        }
    }

    private fun createDummyModel() {
        Log.d(TAG, "ü§ñ Criando modelo dummy para demonstra√ß√£o...")
        // Aqui voc√™ poderia criar um modelo simples para teste
        // Por enquanto, s√≥ registra que est√° funcionando sem modelo
        showToast("Modo demonstra√ß√£o: apenas detec√ß√£o de faces")
    }

    private fun listAssetsFiles() {
        try {
            val files = assets.list("") ?: emptyArray()
            Log.d(TAG, "üìÅ Arquivos na pasta assets (${files.size}):")

            if (files.isEmpty()) {
                Log.w(TAG, "   üìÇ Pasta vazia!")
            } else {
                files.forEachIndexed { index, file ->
                    val size = try {
                        assets.openFd(file).declaredLength
                    } catch (e: Exception) {
                        -1L
                    }
                    Log.d(TAG, "   ${index + 1}. $file (${if (size >= 0) "${size} bytes" else "tamanho desconhecido"})")
                }
            }

            Log.d(TAG, "üîç Procurando especificamente por model.tflite...")
            val hasModel = files.contains("model.tflite")
            Log.d(TAG, "   model.tflite presente: ${if (hasModel) "‚úÖ SIM" else "‚ùå N√ÉO"}")

        } catch (e: Exception) {
            Log.e(TAG, "Erro ao listar assets", e)
        }
    }

    private fun checkAndExtractModelDimensions(): Boolean {
        return try {
            val interp = interpreter ?: return false

            val inputTensor = interp.getInputTensor(0)
            val outputTensor = interp.getOutputTensor(0)

            val inputShape = inputTensor.shape()
            val outputShape = outputTensor.shape()

            Log.d(TAG, "üìä === DIMENS√ïES DO MODELO ===")
            Log.d(TAG, "Input shape: ${inputShape.contentToString()}")
            Log.d(TAG, "Output shape: ${outputShape.contentToString()}")
            Log.d(TAG, "Input type: ${inputTensor.dataType()}")
            Log.d(TAG, "Output type: ${outputTensor.dataType()}")

            if (inputShape.size >= 4) {
                modelInputHeight = inputShape[1]
                modelInputWidth = inputShape[2]
                val channels = inputShape[3]
                Log.d(TAG, "üìê Entrada: ${modelInputWidth}x${modelInputHeight}x${channels}")
            }

            if (outputShape.size >= 2) {
                modelOutputSize = outputShape[1]
                Log.d(TAG, "üìê Sa√≠da: vetor ${modelOutputSize}D")
            }

            val valid = modelInputWidth > 0 && modelInputHeight > 0 && modelOutputSize > 0
            Log.d(TAG, "‚úÖ Modelo ${if (valid) "V√ÅLIDO" else "INV√ÅLIDO"}")

            valid

        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro ao analisar modelo", e)
            false
        }
    }

    private fun showToast(message: String) {
        runOnUiThread {
            Toast.makeText(this, message, Toast.LENGTH_LONG).show()
        }
    }

    private fun showSuccessScreen() {
        val usuario = intent.getSerializableExtra("usuario") as? FuncionariosLocalModel
        val faceBitmap = currentFaceBitmap
        
        if (faceBitmap != null) {
            // Criar uma vers√£o otimizada para exibi√ß√£o (300x300 para melhor qualidade)
            val displayBitmap = Bitmap.createScaledBitmap(faceBitmap, 300, 300, true)
            
            // Armazenar no TempImageStorage para evitar problema de transa√ß√£o
            TempImageStorage.storeFaceBitmap(displayBitmap)
            
            // Abre a tela de confirma√ß√£o
            FaceRegistrationSuccessActivity.start(this, usuario)
            finish() // Fecha a CameraActivity
        } else {
            // Fallback para toast se n√£o tiver a foto
            showToast("‚úÖ Facial cadastrado com sucesso!")
        }
    }

    private fun startCamera() {
        Log.d(TAG, "üì∑ === INICIANDO C√ÇMERA ===")

        val cameraProviderFuture = ProcessCameraProvider.getInstance(this)
        cameraProviderFuture.addListener({
            val cameraProvider = cameraProviderFuture.get()
            val preview = Preview.Builder().build().also {
                it.setSurfaceProvider(previewView.surfaceProvider)
            }

            imageAnalyzer = ImageAnalysis.Builder()
                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
                .setTargetResolution(android.util.Size(640, 480)) // Resolu√ß√£o otimizada para velocidade
                .build().also {
                    it.setAnalyzer(ContextCompat.getMainExecutor(this)) { proxy ->
                        processImage(proxy)
                    }
                }

            try {
                cameraProvider.unbindAll()
                cameraProvider.bindToLifecycle(
                    this,
                    CameraSelector.DEFAULT_FRONT_CAMERA,
                    preview,
                    imageAnalyzer
                )
                Log.d(TAG, "‚úÖ C√¢mera ativa!")
            } catch (e: Exception) {
                Log.e(TAG, "‚ùå Erro na c√¢mera", e)
                showToast("Erro na c√¢mera: ${e.message}")
            }
        }, ContextCompat.getMainExecutor(this))
    }

    private fun processImage(imageProxy: ImageProxy) {
        val mediaImage = imageProxy.image
        if (mediaImage != null) {
            val image = InputImage.fromMediaImage(mediaImage, imageProxy.imageInfo.rotationDegrees)

            faceDetector.process(image)
                .addOnSuccessListener { faces ->
                    if (faces.isNotEmpty()) {
                        faceDetectionCount++
                        val face = faces[0]

                        Log.d(TAG, "üë§ FACE #$faceDetectionCount detectada!")

                        overlay.setBoundingBox(face.boundingBox, mediaImage.width, mediaImage.height)

                        // ‚úÖ S√≥ salva se estiver no centro
                        if (!alreadySaved && overlay.isFaceInOval(face.boundingBox)) {
                            processDetectedFace(mediaImage, face.boundingBox)
                            alreadySaved = true
                            showToast("‚úÖ Rosto centralizado e salvo com sucesso!")
                        }
                    } else {
                        overlay.clear()
                    }
                    imageProxy.close()
                }
                .addOnFailureListener { e ->
                    Log.e(TAG, "‚ùå Erro na detec√ß√£o", e)
                    imageProxy.close()
                }
        } else {
            imageProxy.close()
        }
    }


    private fun processDetectedFace(mediaImage: android.media.Image, boundingBox: Rect) {
        try {
            Log.d(TAG, "üîÑ === PROCESSANDO FACE ===")

            val bitmap = toBitmap(mediaImage)
            saveImage(bitmap, "original")

            val faceBmp = cropFace(bitmap, boundingBox)
            saveImage(faceBmp, "face_cropped")

            val resized = Bitmap.createScaledBitmap(faceBmp, modelInputWidth, modelInputHeight, true)
            saveImage(resized, "face_${modelInputWidth}x${modelInputHeight}")

            // Manter orienta√ß√£o natural da imagem
            val correctedFace = faceBmp

            // Salvar a foto do rosto para mostrar na tela de confirma√ß√£o (tamanho otimizado para melhor qualidade)
            val faceForDisplay = Bitmap.createScaledBitmap(correctedFace, 300, 300, true)
            currentFaceBitmap = fixImageOrientationDefinitive(faceForDisplay) // Corrigir orienta√ß√£o
            
            Log.d(TAG, "üíæ Imagens salvas com corre√ß√£o de orienta√ß√£o!")

            if (modelLoaded && interpreter != null) {
                executeInference(resized)
            } else {
                Log.d(TAG, "‚ö†Ô∏è  Sem modelo - apenas detec√ß√£o")
                // Mostrar tela de sucesso mesmo sem modelo
                showSuccessScreen()
            }

        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro no processamento", e)
            showToast("Erro: ${e.message}")
        }
    }

    private fun executeInference(bitmap: Bitmap) {
        try {
            Log.d(TAG, "üß† === EXECUTANDO INFER√äNCIA ===")

            // Usar o mesmo m√©todo de convers√£o que a PontoActivity
            val inputTensor = convertBitmapToTensorInput(bitmap)
            val output = Array(1) { FloatArray(modelOutputSize) }

            interpreter?.run(inputTensor, output)

            val vetorFacial = output[0]
            val usuario = intent.getSerializableExtra("usuario") as? FuncionariosLocalModel

            if (usuario != null) {
                // Primeiro deletar face antiga se existir
                CoroutineScope(Dispatchers.IO).launch {
                    try {
                        val dao = AppDatabase.getInstance(applicationContext).faceDao()
                        
                        // Deletar face antiga
                        dao.deleteByFuncionarioId(usuario.codigo)
                        Log.d(TAG, "üóëÔ∏è Face antiga deletada para funcion√°rio ${usuario.codigo}")
                        
                        // Criar nova face
                        val faces = FaceEntity(
                            id = 0, // Deixar o Room gerar o ID
                            funcionarioId = usuario.codigo,
                            embedding = vetorFacial.joinToString(","),
                            synced = true
                        )
                        
                        // Inserir nova face
                        dao.insert(faces)
                        Log.d(TAG, "‚úÖ Nova face salva para funcion√°rio ${usuario.codigo}")
                        
                        // Mostrar tela de confirma√ß√£o na thread principal
                        withContext(Dispatchers.Main) {
                            showSuccessScreen()
                        }
                    } catch (e: Exception) {
                        Log.e(TAG, "‚ùå Erro ao salvar face: ${e.message}")
                        withContext(Dispatchers.Main) {
                            showToast("Erro ao salvar face: ${e.message}")
                        }
                    }
                }


            } else {
                Log.e(TAG, "‚ùå Usuario nulo - n√£o foi poss√≠vel salvar o vetor facial.")
                showToast("Erro: usu√°rio n√£o encontrado.")
            }
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro na infer√™ncia", e)
            showToast("Erro na infer√™ncia: ${e.message}")
        }
    }


    private fun convertBitmapToTensorInput(bitmap: Bitmap): ByteBuffer {
        try {
            val inputSize = modelInputWidth // 112
            Log.d(TAG, "üîß Preparando tensor para entrada ${inputSize}x${inputSize}")
            
            if (bitmap.isRecycled) {
                throw IllegalStateException("Bitmap foi reciclado")
            }
            
            // Alocar buffer com tamanho correto para float32
            val byteBuffer = ByteBuffer.allocateDirect(4 * inputSize * inputSize * 3)
            byteBuffer.order(ByteOrder.nativeOrder())

            val resizedBitmap = if (bitmap.width != inputSize || bitmap.height != inputSize) {
                Log.d(TAG, "üîß Redimensionando bitmap de ${bitmap.width}x${bitmap.height} para ${inputSize}x${inputSize}")
                Bitmap.createScaledBitmap(bitmap, inputSize, inputSize, true)
            } else {
                bitmap
            }
            
            val intValues = IntArray(inputSize * inputSize)
            resizedBitmap.getPixels(intValues, 0, inputSize, 0, 0, inputSize, inputSize)
            Log.d(TAG, "‚úÖ Pixels extra√≠dos: ${intValues.size} pixels")

            var pixelCount = 0
            for (pixel in intValues) {
                // Normalizar para [-1, 1] como esperado pelo modelo MobileFaceNet
                val r = ((pixel shr 16) and 0xFF) / 127.5f - 1.0f
                val g = ((pixel shr 8) and 0xFF) / 127.5f - 1.0f
                val b = (pixel and 0xFF) / 127.5f - 1.0f

                byteBuffer.putFloat(r)
                byteBuffer.putFloat(g)
                byteBuffer.putFloat(b)
                pixelCount++
            }
            
            Log.d(TAG, "‚úÖ Tensor preenchido com $pixelCount pixels")
            
            // Limpar bitmap tempor√°rio se foi criado
            if (resizedBitmap != bitmap) {
                resizedBitmap.recycle()
            }

            return byteBuffer
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro em convertBitmapToTensorInput", e)
            throw e
        }
    }

    private fun saveImage(bitmap: Bitmap, prefix: String) {
        try {
            val timestamp = SimpleDateFormat("yyyyMMdd_HHmmss", Locale.getDefault()).format(Date())
            val filename = "${prefix}_${timestamp}.jpg"

            val picturesDir = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES)
            val appDir = File(picturesDir, "FaceApp")
            appDir.mkdirs()
            val file = File(appDir, filename)

            FileOutputStream(file).use { out ->
                bitmap.compress(Bitmap.CompressFormat.JPEG, 90, out)
            }

            Log.d(TAG, "üíæ Salvo: ${file.name}")

        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erro ao salvar", e)
        }
    }

    private fun allPermissionsGranted() = REQUIRED_PERMISSIONS.all {
        ContextCompat.checkSelfPermission(baseContext, it) == PackageManager.PERMISSION_GRANTED
    }

    @Throws(IOException::class)
    private fun loadModelFile(fileName: String): ByteBuffer {
        return assets.openFd(fileName).use { fileDescriptor ->
            FileInputStream(fileDescriptor.fileDescriptor).use { inputStream ->
                val fileChannel = inputStream.channel
                val startOffset = fileDescriptor.startOffset
                val declaredLength = fileDescriptor.declaredLength

                fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
            }
        }
    }

    override fun onDestroy() {
        super.onDestroy()
        interpreter?.close()
        Log.d(TAG, "üõë App finalizado")
    }

    override fun onRequestPermissionsResult(
        requestCode: Int,
        permissions: Array<String>,
        grantResults: IntArray
    ) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults)
        if (requestCode == REQUEST_CODE_PERMISSIONS) {
            if (allPermissionsGranted()) {
                startCamera()
            } else {
                // Verificar quais permiss√µes foram negadas
                val deniedPermissions = mutableListOf<String>()
                for (i in permissions.indices) {
                    if (grantResults[i] != PackageManager.PERMISSION_GRANTED) {
                        deniedPermissions.add(permissions[i])
                    }
                }
                
                Log.e(TAG, "‚ùå Permiss√µes negadas: ${deniedPermissions.joinToString(", ")}")
                
                                    val message = when {
                        deniedPermissions.contains(Manifest.permission.CAMERA) -> 
                            "‚ùå Permiss√£o de c√¢mera negada!\n\nPara registrar sua face, voc√™ precisa permitir o acesso √† c√¢mera.\n\nV√° em Configura√ß√µes > Apps > iFace Offline > Permiss√µes e ative a c√¢mera."
                        deniedPermissions.contains(Manifest.permission.READ_MEDIA_IMAGES) -> 
                            "‚ùå Permiss√£o de m√≠dia negada!\n\nPara salvar fotos, voc√™ precisa permitir o acesso √†s imagens.\n\nV√° em Configura√ß√µes > Apps > iFace Offline > Permiss√µes e ative 'Fotos e v√≠deos'."
                        deniedPermissions.contains(Manifest.permission.POST_NOTIFICATIONS) -> 
                            "‚ùå Permiss√£o de notifica√ß√£o negada!\n\nPara receber avisos do app, voc√™ precisa permitir notifica√ß√µes.\n\nV√° em Configura√ß√µes > Apps > iFace Offline > Permiss√µes e ative 'Notifica√ß√µes'."
                        deniedPermissions.contains(Manifest.permission.WRITE_EXTERNAL_STORAGE) -> 
                            "‚ùå Permiss√£o de armazenamento negada!\n\nPara salvar fotos, voc√™ precisa permitir o acesso ao armazenamento.\n\nV√° em Configura√ß√µes > Apps > iFace Offline > Permiss√µes e ative 'Armazenamento'."
                        else -> "‚ùå Permiss√µes necess√°rias foram negadas!\n\nV√° em Configura√ß√µes > Apps > iFace Offline > Permiss√µes e ative todas as permiss√µes."
                    }
                
                Toast.makeText(this, message, Toast.LENGTH_LONG).show()
                
                // Aguardar um pouco antes de fechar para o usu√°rio ler a mensagem
                android.os.Handler(android.os.Looper.getMainLooper()).postDelayed({
                    finish()
                }, 3000)
            }
        }
    }
}